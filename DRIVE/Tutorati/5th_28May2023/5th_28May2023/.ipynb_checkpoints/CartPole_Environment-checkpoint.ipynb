{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> CartPole, familiarizing with the environment\n",
    "    \n",
    "The exercize is based on one environment taken from on the **Gym** library, a toolkit for developing and comparing reinforcement learning algorithms:\n",
    "    \n",
    "https://gym.openai.com/envs/CartPole-v1/\n",
    "    \n",
    "### Description\n",
    "\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a 1-dim frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n",
    "\n",
    "This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson (example 3.4, 2018 version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true content of the class can be seen here: https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py . The basic structure follows what we already had in the past exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeCartPole():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Fake Class similar to CartPole which does NOTHING.\n",
    "        \"\"\"\n",
    "        # Definition of State Space\n",
    "        \n",
    "        # Definition of Action Space\n",
    "        \n",
    "        # Definition of \"Physics\" of the problem\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the env.\n",
    "        \"\"\"\n",
    "        # Reset the environment to initial state\n",
    "        pass\n",
    "        \n",
    "    def step(self, A):\n",
    "        \"\"\"\n",
    "        Evolves the environment given action A which is application of force to left or right\n",
    "        \"\"\"\n",
    "        # ----\n",
    "        # actual evolution\n",
    "        # ----\n",
    "        return new_state, reward, done, info\n",
    "        \n",
    "        \n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Does nothing.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    # PLUS A COUPLE OF OTHERS...\n",
    "    def seed(self):\n",
    "        \"\"\"\n",
    "        For random seed\n",
    "        \"\"\"\n",
    "        # STUFF\n",
    "        pass\n",
    "        \n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        To close the environment\n",
    "        \"\"\"\n",
    "        # STUFF\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State space\n",
    "The state space is a Box(4) object, which is a 4-dimensional vector of floats.\n",
    "Each dimension can be bounded, and represents the following observables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space:  Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
      "\n",
      "1st element:\tPosition of the cart along the x-axis. Bounds: [-4.8, 4.8]\n",
      "2nd element:\tCart velocity. Not bounded\n",
      "3rd element:\tPole angle. Bounds: [-24.0, 24.0]\n",
      "4th element:\tPole velocity at its tip. Not bounded\n"
     ]
    }
   ],
   "source": [
    "print(\"State space: \", env.observation_space)\n",
    "print()\n",
    "\n",
    "low_bounds, high_bounds = (env.observation_space.low, env.observation_space.high)\n",
    "print(\"1st element:\\tPosition of the cart along the x-axis. Bounds: [%2.1f, %2.1f]\" %(low_bounds[0], high_bounds[0]))\n",
    "print(\"2nd element:\\tCart velocity. Not bounded\")\n",
    "print(\"3rd element:\\tPole angle. Bounds: [%2.1f, %2.1f]\" %(np.rad2deg(low_bounds[2]), np.rad2deg(high_bounds[2])))\n",
    "print(\"4th element:\\tPole velocity at its tip. Not bounded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action space\n",
    "The action space is a discrete space of size two.\n",
    "The two actions are applying a certain fixed force to the cart towards left or right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space:  Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action space: \", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards\n",
    "Reward is 1 for every step taken, including the termination step\n",
    "### Starting State\n",
    "All observations are assigned a uniform random value in [-0.05..0.05]\n",
    "### Episode Termination\n",
    "* Pole Angle is more than 12 degrees\n",
    "* Cart Position is more than 2.4 (center of the cart reaches the edge of the display)\n",
    "* Episode length is greater than 500, or another value set by the variable *env.\\_max\\_episode\\_steps*\n",
    "* Solved Requirements (considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How the environment works\n",
    "The key methods of the gym environments are reset() and step(action).\n",
    "\n",
    "As the name suggests, the first reset the environment. It also returns the starting state following the rule written above (all the 4 values are assiged at random within a given window)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step(action) method returns the next state as a consequence of the action (pushing the cart rigth or left).\n",
    "Calling this method the env engine has to compute the physics of the system. \n",
    "Together with the new state, step(action) returns also the obtained reward and whether the episode has reached a terminal state (the fourth variable info will not be considered here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The action 0 corresponds to the left pushing\n",
    "new_state, reward, done, info = env.step(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you now compare the two state, you can see that the cart has decreased its velocity (second element of the vector) as a consequence of the left pushing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02344187 -0.03038084  0.02490161 -0.0284238 ]\n",
      "[-0.02404948 -0.22585088  0.02433314  0.27201068]\n"
     ]
    }
   ],
   "source": [
    "print(starting_state)\n",
    "print(new_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying a naive strategy\n",
    "One can define a naive strategy (i.e. the action to take by knowing the current state) based on the physical intuition of the problem. \n",
    "\n",
    "Remember that the best performance is having an episode cumulative reward of 500, because after 500 steps the environment automatcally resets (see episode termination above).\n",
    "Having a smaller reward means that the episode has ended before 500 steps because (1) the angle of the pole has become too large, (2) the cart is outside the boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of possible policies. Method which returns an action given the state as argument\n",
    "\n",
    "def my_bad_policy(state):\n",
    "    \"\"\"\n",
    "    If the pole angle is less than 0 (bent towards left) I apply a force towards left.\n",
    "    \"\"\"\n",
    "    if state[2] < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 \n",
    "    \n",
    "def random_policy(state):\n",
    "    return np.random.randint(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main cycle for running the environemnt\n",
    "\n",
    "def run(env, n_episodes, strategy, render=False):\n",
    "    \"\"\"\n",
    "    Running the enviroment for a given number of episodes, according to a given strategy.\n",
    "    It returns the average reward over all the episodes.\n",
    "    \"\"\"\n",
    "    average_reward = 0 # Cumulative reward averaged over all the episodes\n",
    "    \n",
    "    for _ in range(n_episodes): # Cycle over all the episodes\n",
    "        \n",
    "        state = env.reset() # Episode initiaization\n",
    "        ep_reward = 0\n",
    "        \n",
    "        while True: # Cycle over the steps\n",
    "            \n",
    "            if render:\n",
    "                env.render() # This method can render the environment\n",
    "                \n",
    "            action = strategy(state) # Getting the action from the heuristic policy\n",
    "            state, reward, done, info = env.step(action) # Environmental step\n",
    "            ep_reward += reward\n",
    "            \n",
    "            if done: # Check if the state is terminal\n",
    "                break\n",
    "                \n",
    "        average_reward += ep_reward / float(n_episodes)\n",
    "        \n",
    "    return average_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the average reward of the heuristic policy over some episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.08999999999998\n",
      "22.332000000000004\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 500\n",
    "\n",
    "print (run(env, n_episodes, my_bad_policy))\n",
    "print (run(env, n_episodes, random_policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you come up with a better strategy? \n",
    "\n",
    "Reaching the best performance with just intuition, is almost impossible.. We can approach the problem with **reinforcement learning**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendering\n",
    "The gym environment provides also the possibility to visualize how the system evolve throught the render method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0896227f3e89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_bad_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-36481c182df2>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(env, n_episodes, strategy, render)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This method can render the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Getting the action from the heuristic policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     raise ImportError('''\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mcompat_platform\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'darwin'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcocoa\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCocoaConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mbase\u001b[0m  \u001b[0;31m# noqa: F821\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
     ]
    }
   ],
   "source": [
    "run(env, 100, my_bad_policy, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
