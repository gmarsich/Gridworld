{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dueling Agents in Games\n",
    "\n",
    "We consider now the framework of Game Theory, which studies the strategic interactions between players.\n",
    "Instead of our usual environments, we will use _normal-form games_ between (in our particular case) two players.\n",
    "\n",
    "The game is defined by the action set of the two players:  $\\mathcal{A}_1$ and $\\mathcal{A}_2$, and by the payoff function $\\mathbf{u}$, which associates the joint actions of the two players to the payoffs (rewards) of each players.\n",
    "Players select their actions simultaneously, without knowing the selection of the other player, and the rewards are calculated.\n",
    "\n",
    "When the number of actions and players is small it is convenient to write payoff functions as _matrices_.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\text {2P, 2A payoff matrix }\\\\\n",
    "&\\begin{array}{cccc}\n",
    "\\hline \\hline  &  &  \\text{Player 2} \\\\\n",
    "\\hline  &  & A  & B \\\\\n",
    "\\text{Player 1} & A & (R_{AA}^1, R_{AA}^2) & (R_{AB}^1, R_{AB}^2) \\\\\n",
    " & B & (R_{BA}^1, R_{BA}^2) & (R_{BB}^1, R_{BB}^2) \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So, if player $1$ plays action $A$, and player $2$ plays action $B$, they receive $R_{AB}^1$ and $R_{AB}^2$ respectively.\n",
    "Depending on the matrices, the game can be, for example:\n",
    "\n",
    "- \"simmetric\" (if $R_{XY}^1 = R_{YX}^2$ for any action tuple $X,Y$)\n",
    "- zero-sum (if $R_{XY}^1 + R_{XY}^2 = 0$ for any action tuple $X,Y$)\n",
    "- ...\n",
    "\n",
    "As in the case of simple bandits, there is no \"dynamics\": actions do not change the state of the game (i.e. the payoff matrix).\n",
    "However, even if there is no real dynamic, the system - as perceived by one player - is changing all the time! Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Strategy\"\n",
    "\n",
    "The strategy of any player (agent, in RL terms) is defined by the probability to select the different actions (policy, in RL terms). In the dueling setup for MultiAgents, two players play game after game one versus the other, and change their strategy in response to the actions selected by the other player."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rock-Paper-Scissors (RPS)\n",
    "\n",
    "There is no real need to create an entire class: the game is defined only by the payoff matrix!\n",
    "Let's create the payoff matrix of the rock-paper-scissors game.\n",
    "\n",
    "Reminder: it is a child game which is \n",
    "- zero-sum (at all times either one player wins (r=+1) and the other loses (r=-1), or both draw (r=0). \n",
    "- Rock beats Scissors (1 v -1), Scissors beats Paper (1 v -1), Paper beats Rock (1 v -1). If two players choose the same, it is a tie (0 v 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a payoff matrix for the game of Rock Paper Scissors!\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "Payoff = np.array([...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize a policy for RPS\n",
    "\n",
    "What is \"a strategy\" in this game? How can we visualize it?\n",
    "We have prob(Rock), prob(Paper) and prob(Scissors). However, only two are independent, since clearly one can be written in terms of the other two prob(Scissors) = 1 - prob(Rock) - prob(Paper).\n",
    "\n",
    "In $3D$ therefore we can plot all possible strategies as a \"triangle\", where each point correspond to a specific strategy and the corners are the \"pure\" strategy (i.e. 100% of doing one action).\n",
    "\n",
    "(The space in $\\mathcal{R}^d$ which correspond to all possible probabilities is called a simplex $\\Delta^d$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f5ca5902d30>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How to plot policies?\n",
    "\n",
    "plt.title('Symplex in 3D')\n",
    "\n",
    "plt.xlim(-0.6,0.6)\n",
    "plt.ylim(-0.1,np.sqrt(3)/2+0.1)\n",
    "\n",
    "plt.tick_params(\n",
    "    axis='both',          # changes apply to the x-axis\n",
    "    left=False,\n",
    "    right=False,\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelleft=False, # labels along the bottom edge are off\n",
    "    labelbottom=False) # labels along the bottom edge are off\n",
    "\n",
    "plt.text(-0.02, np.sqrt(3)/2+0.02,'S')\n",
    "plt.text(-0.52, -0.05,'R')\n",
    "plt.text(0.48, -0.05,'P')\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.plot([-0.5, 0.5, 0, -0.5], [0,0,np.sqrt(3)/2,0])\n",
    "\n",
    "# What about a single policy?\n",
    "#    P(R), P(P), P(S)\n",
    "pi = [0.2,  0.3, 0.5]\n",
    "\n",
    "# This policy maps to a (x,y), such that:\n",
    "# x = (P(P) - P(R)) / 2\n",
    "# y = P(S)\n",
    "pi_projected = ((pi[1] - pi[0])/2 , pi[2] * np.sqrt(3)/2)\n",
    "\n",
    "plt.text(*pi_projected, 'One single policy')\n",
    "\n",
    "plt.scatter(*pi_projected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we go from a trajectory for RockPaperScissor policies to its projection for plot.\n",
    "def project_pi(traj_pi):\n",
    "    pi_x_projected = (traj_pi[:,1] - traj_pi[:,1])/2\n",
    "    pi_y_projected = traj_pi[:,2]*np.sqrt(3)/2\n",
    "    return pi_x_projected, pi_y_projected\n",
    "\n",
    "# Utility to plot trajectories\n",
    "def plot_traj(trajs, labels):\n",
    "\n",
    "    proj_trajs = []\n",
    "    for traj in trajs:\n",
    "        x_proj, y_proj = project_pi(traj)\n",
    "        proj_trajs.append([x_proj, y_proj])\n",
    "        \n",
    "    plt.title('Trajectory of policies')\n",
    "\n",
    "    ylim = -0.1\n",
    "    if labels:\n",
    "        ylim -= 0.1*len(labels)\n",
    "    \n",
    "    plt.xlim(-0.6,0.6)\n",
    "    plt.ylim(ylim,np.sqrt(3)/2+0.1)\n",
    "\n",
    "    plt.tick_params(\n",
    "        axis='both',          # changes apply to the x-axis\n",
    "        left=False,\n",
    "        right=False,\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelleft=False, # labels along the bottom edge are off\n",
    "        labelbottom=False) # labels along the bottom edge are off\n",
    "\n",
    "    plt.text(-0.02, np.sqrt(3)/2+0.02,'S')\n",
    "    plt.text(-0.52, -0.05,'R')\n",
    "    plt.text(0.48, -0.05,'P')\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.plot([-0.5, 0.5, 0, -0.5], [0,0,np.sqrt(3)/2,0])\n",
    "\n",
    "    if labels:\n",
    "        for proj_traj, label in zip(proj_trajs, labels):\n",
    "            plt.plot(proj_traj[0], proj_traj[1], label=label)\n",
    "            plt.scatter(proj_traj[0][-1], proj_traj[1][-1])\n",
    "        plt.legend(loc='lower left')\n",
    "    else:\n",
    "        for proj_traj in proj_trajs:\n",
    "            plt.plot(proj_traj[0], proj_traj[1])\n",
    "            plt.scatter(proj_traj[0][-1], proj_traj[1][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an agent (standard gradient)\n",
    "\n",
    "The individual player is a RL agent, which acts in a (changing) environment, which depends on the actions of the \n",
    "\n",
    "Therefore, we will use a RL algorithm among those we have already written. Which one?\n",
    "\n",
    "Q: Can we use model-based algorithm? (Do we know exactly the model?)\n",
    "\n",
    "#### What are the characteristics of the agent?\n",
    "\n",
    "- Number of states?\n",
    "- Policy?\n",
    "- Episode length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentForDuel():\n",
    "    # Create one, using one of the old classes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dueling setup.\n",
    "\n",
    "Now, let's put one agent against another, and let's make learn simultaneously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Payoff = ...\n",
    "Nplays = 10000\n",
    "\n",
    "P1 = AgentForDuel()\n",
    "P2 = AgentForDuel()\n",
    "\n",
    "traj_policy_P1 = np.empty(3,)\n",
    "traj_policy_P2 = np.empty(3,)\n",
    "\n",
    "for i in range (N_plays):\n",
    "    \n",
    "    a1 = ... # Select random action from Player1\n",
    "    a2 = ... # Select random action from Player2\n",
    "    \n",
    "    r1 = ... # Select payoff for P1 given a1, a2\n",
    "    r2 = ... # Select payoff for P2 given a1, a2\n",
    "\n",
    "    # Update policy for P1\n",
    "    # Update policy for P2\n",
    "    \n",
    "    # Record policy of P1\n",
    "    # Record policy of P2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the trajectory in policy space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_traj([traj_policy_P1, traj_policy_P2], ['Standard Gradient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an agent (normal gradient)\n",
    "\n",
    "#### (Optional: Intuitive theory of normal gradient. Otherwise, just go to practice\n",
    "Now, let's use the concept of *normal* gradient. Where does it come from?\n",
    "\n",
    "Let's recall the main idea of gradient based minimization.\n",
    "In general, we have a function $f(x)$ we want to minimize, we are at some point $x_t$ and we have an estimate of the gradient there $ \\partial_x f(x_t)$. Then, we can construct an approximation of the function:\n",
    "\n",
    "$$\n",
    "f_{approx}(x) = f(x_t) + \\partial_x f(x_t) (x-x_t)\n",
    "$$\n",
    "\n",
    "Clearly, if we minimize that, we would move infinitely in the direction of the gradient. This is wrong, because generally, the linear approximation is valid only _close_ to $x_t$. Therefore, we minimize a different term:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(x) = f(x_t) + \\partial_x f(x_t) (x-x_t) + \\frac{1}{2\\alpha} ( x - x_t)^2\n",
    "$$\n",
    "\n",
    "The term $\\frac{1}{2\\alpha} ( x - x_t)^2$ tells us \"not to go too far\" away from $x_t$!\n",
    "\n",
    "Putting $\\partial_x \\mathcal{L}(x) = 0$, we get the typical update rule:\n",
    "\n",
    "$$\n",
    "x = x_t - \\alpha \\partial_x f(x_t)\n",
    "$$\n",
    "\n",
    "\n",
    "#### How do we apply that to PG?\n",
    "\n",
    "Now, we don't have a $f(x)$, however we have an expected return $\\mathcal{J}$ which is a function of the policy $\\pi$ _through_ the parameters $\\theta$. The standard update, would then correspond to the minimization of:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\pi | \\theta) = \\mathcal{J}(\\pi | \\theta_t) + \\nabla_\\theta \\mathcal{J}(\\pi | \\theta_t) (\\theta - \\theta_t) + \\frac{1}{2\\alpha} || \\theta-\\theta_t|| ^2\n",
    "$$\n",
    "\n",
    "*However*, we now can see that there is an interesting thing. The equation above tells us, \"change $\\theta$ in the direction of the gradient, but don't change $\\theta$ too much, else the gradient will be wrong!\". \n",
    "\n",
    "However, the function of interest $\\mathcal{J}$ and its gradient does not depend on $\\theta$ directly, but on the _policies!_. It may well be that $\\theta$ change A LOT, while the gradient remains the same! \n",
    "\n",
    "So, it should really say: \"change $\\theta$ in the direction of the gradient, but don't change *the policy $\\pi$$ too much!\". So we just need some measure of \"distance\" between policies, which is the Kullback-Leibler divergence.\n",
    "\n",
    "$$\n",
    "|| \\theta-\\theta_t||^2 \\longrightarrow D_{KL}(\\, \\pi_\\theta \\, || \\, \\pi_{\\theta_t} \\, )\n",
    "$$\n",
    "\n",
    "So we want to \"move as quickly as possible in the direction of $\\theta$ where policies do not change\" and be \"move cautiously\" in directions where policies change fast.\n",
    "It turns out that we can achieve this if instead of the gradient $\\nabla$ we follow the **natural gradient** $\\tilde{\\nabla} = F[\\theta]^{-1} \\cdot \\nabla$, where $F[\\theta]$ is the Fisher Matrix, defined as:\n",
    "\n",
    "$$\n",
    "F[\\theta]_{i,j} = \\sum_a \\pi(a) \\frac{\\partial \\text{log} \\pi(a)}{\\partial \\theta_i} \\frac{\\partial \\text{log}\\pi(a)}{\\partial \\theta_j}\n",
    "$$\n",
    "\n",
    "### Yes, ok... But how, in practice?\n",
    "\n",
    "Let us consider the parametrization of the policies used so far, with $N$ parameters $(\\theta_1, \\theta_2, \\dots, \\theta_N)$ for $N$ actions. (Ignoring for simplicity the dependence on the state). \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\pi(a | \\theta) &=& \\pi(a) &= \\frac{e^{\\theta_a}}{\\sum_j e^{\\theta_j}} \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Let us recall that\n",
    "$$\n",
    "    \\nabla_i \\text{log}\\pi(a) = \\frac{\\partial \\text{log } \\pi(a)}{\\partial \\theta_i} = \\left\\{\n",
    "    \\begin{array}{clll}\n",
    "        & 1-\\pi(i)  \\,\\,\\,& \\text{if} \\,\\, a = i \\\\[2ex]\n",
    "        & - \\pi(i) \\,\\,\\,& \\text{if} \\,\\, a \\neq i \\\\[2ex]\n",
    "\\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "In a more concise way, we have that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{log } \\pi(a)}{\\partial \\theta_i} = \\mathbb{1}_{(a=i)} - \\pi(i) \\\\\n",
    "$$\n",
    "\n",
    "Now we can construct the Fisher Information Matrix. The $(i,j)^{th}$ element of the matrix is:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "F[\\theta]_{i,j} &=& \\sum_a \\pi_a \\frac{\\partial \\text{log} \\pi(a)}{\\partial \\theta_i} \\frac{\\partial \\text{log}\\pi(a)}{\\partial \\theta_j} \\\\\n",
    "&=& \\sum_a \\pi(a) \\big(\\mathbb{1}_{(a=i)} - \\pi(i)\\big)\\big(\\mathbb{1}_{(a=j)} - \\pi(j)\\big) \\\\\n",
    "&=& \\sum_a \\pi(a) \\big(\\mathbb{1}_{(a=i)}\\mathbb{1}_{(a=j)} - \\mathbb{1}_{(a=j)}\\pi(i) - \\mathbb{1}_{(a=i)}\\pi(j) + \\pi(j)\\pi(i)\\big) \\\\\n",
    "&=& \\pi(i) \\mathbb{1}_{(i=j)} - \\pi(j)\\pi(i)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "One more step is needed!\n",
    "As we write the natural gradient $\\tilde{\\nabla} = F[\\theta]^-1 \\nabla$ we can equivalently express the standard gradient as $F[\\theta] \\tilde{\\nabla} = \\nabla$. \n",
    "Expressing the matrix product explicitly for a single derivative $\\partial/\\partial{\\theta_i} $ we have:\n",
    "\n",
    "$$\n",
    "\\sum_j F[\\theta]_{i,j} \\tilde{\\nabla}_j = \\nabla_i = \\partial/\\partial{\\theta_i}\n",
    "$$\n",
    "\n",
    "Let us now apply the natural gradient to $\\text{log}(\\pi(a))$ (where $a$ is the action which enters in the Action-Critic update):\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\sum_j F[\\theta]_{i,j} \\tilde{\\nabla}_j \\text{log}(\\pi(a))&=& \\partial/\\partial{\\theta_i} \\text{log}(\\pi(a))\\\\\n",
    "\\sum_j (\\pi(i) \\mathbb{1}_{(i=j)} - \\pi(j)\\pi(i)) \\left[ \\tilde{\\nabla}_j \\text{log}(\\pi(a)) \\right] &=& \\mathbb{1}_{(a=i)} - \\pi(i)\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Solving the equation above, we can finally find the explicit expression for the natural gradient in the case of the soft-max policies.\n",
    "\n",
    "$$\n",
    "\\tilde{\\nabla}_j \\text{log}(\\pi(a)) = \\frac{\\mathbb{1}_{(a=j)}}{\\pi(j)} \n",
    "$$\n",
    "\n",
    "Notice what happens when $\\pi \\rightarrow 0$. The standard gradient goes also to $0$, and therefore the value $\\theta$ is \"stuck\" there. However for the natural gradient if $\\pi \\rightarrow 0$ the corresponding update on $\\theta$ can be potentially unbounded!\n",
    "\n",
    "# End of the theory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an agent with Natural Gradient!\n",
    "\n",
    "Does everything seem too overwhelming? No worries!\n",
    "The modifications are actually super simple to make!\n",
    "\n",
    "The only thing to change is the update. Instead of the \"standard gradient\", \n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{log } \\pi(a)}{\\partial \\theta_b} = \\mathbb{1}_{(a=b)} - \\pi(b) \\\\\n",
    "$$\n",
    "\n",
    "We will have to code the \"natural gradient\" for softmax, which is:\n",
    "\n",
    "$$\n",
    "\\frac{\\tilde{\\partial} \\text{log } \\pi(a)}{\\partial \\theta_b} = \\frac{\\mathbb{1}_{(a=b)}}{\\pi(b)} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentForDuel_naturalGradient():\n",
    "    # COPY AND PASTE the previous agent\n",
    "    # CHANGE the update of the parameter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the dueling as before, but using the natural gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Payoff = ...\n",
    "Nplays = 10000\n",
    "\n",
    "P1 = AgentForDuel_naturalGradient()\n",
    "P2 = AgentForDuel_naturalGradient()\n",
    "\n",
    "# COPY AND PASTE \n",
    "traj_policy_P1_NG = np.empty(3,)\n",
    "traj_policy_P2_NG = np.empty(3,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now let's plot the trajectories in the policy space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_traj([traj_policy_P1_NG, traj_policy_P2_NG], ['Standard Gradient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE:\n",
    "\n",
    "To see a discussion about the replicator dynamics and how they relate to the self-play of a Natural Gradient agent, see the solution notebook!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
